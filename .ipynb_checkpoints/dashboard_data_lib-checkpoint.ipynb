{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mongoDB_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b891620b03e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmongoDB_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspark_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mongoDB_lib'"
     ]
    }
   ],
   "source": [
    "import database_lib.mongoDB_lib as mdb\n",
    "from datetime import date, timedelta\n",
    "import processing_lib.spark_lib as sp\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from collections import defaultdict\n",
    "import database_lib.postgre_lib as pg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_tokens(n):\n",
    "    \n",
    "    today = date.today()\n",
    "    dates = []\n",
    "    for i in range(n):\n",
    "        condition = {\"date\" : str(today - timedelta(days = i))}\n",
    "        dates.append(condition)\n",
    "        \n",
    "    client = mdb.connect_mongoDB()\n",
    "    db = client.TwitterAPI\n",
    "    all_conditions = {\"$or\" : dates}\n",
    "    data = mdb.fetch_data(db, all_conditions)\n",
    "    \n",
    "    return list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tokens(tokens):\n",
    "    token_string = \"{\"\n",
    "    for token in tokens[:-1]:\n",
    "        token_string += token + \", \"\n",
    "    return token_string + tokens[-1] + \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_dataframe(n):\n",
    "\n",
    "    consolidated_dict = defaultdict(list)\n",
    "    data = fetch_recent_tokens(n)\n",
    "    for d in data:\n",
    "        consolidated_dict[d['country']].extend(d['tokens'])\n",
    "    print('Step 1 Completed')\n",
    "        \n",
    "    sc, sqlContext = sp.create_spark_instance()\n",
    "    for key, value in consolidated_dict.items():\n",
    "        print('Spark processing for ' + key)\n",
    "        consolidated_dict[key] = sp.get_consolidated_tokens(sc, value)\n",
    "    \n",
    "    dataframe_list = []\n",
    "    conn, cursor = pg.connect_to_postgre()\n",
    "    for key, value in consolidated_dict.items():\n",
    "        row = []\n",
    "        row.append(str(key))\n",
    "        data = pg.get_records(cursor = cursor, table_name = 'countries', columns = 'latitude, longitude', condition = \"country_name = '\" + key + \"'\")\n",
    "        print(data)\n",
    "        row.append(float(data[0][0]))\n",
    "        row.append(float(data[0][1]))\n",
    "        row.append(format_tokens(value))\n",
    "        dataframe_list.append(row)\n",
    "     \n",
    "    df = pd.DataFrame(dataframe_list, columns = ['Country', 'Latitude', 'Longitude', 'Tokens'])\n",
    "    sc.stop()\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script dashboard_data_lib.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
